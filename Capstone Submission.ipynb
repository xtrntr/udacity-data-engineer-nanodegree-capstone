{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from functools import reduce\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "\n",
    "The goal is to create an ETL pipeline using the Udacity provided I94 immigration dataset and the city temperature data from Kaggle to allow users to make queries to see there is a correlation between destination temperature and immigration statistics.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "\n",
    "The I94 immigration data comes from the US National Tourism and Trade Office. It is provided in SAS7BDAT format which is a binary database storage format. Some relevant attributes include:\n",
    "\n",
    "    i94yr = 4 digit year\n",
    "    i94mon = numeric month\n",
    "    i94cit = 3 digit code of origin city\n",
    "    i94port = 3 character code of destination USA city\n",
    "    arrdate = arrival date in the USA\n",
    "    i94mode = 1 digit travel code\n",
    "    depdate = departure date from the USA\n",
    "    i94visa = reason for immigration\n",
    "\n",
    "The temperature data comes from Kaggle. It is provided in csv format. Some relevant attributes include:\n",
    "\n",
    "    AverageTemperature = average temperature\n",
    "    City = city name\n",
    "    Country = country name\n",
    "    Latitude= latitude\n",
    "    Longitude = longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sas_filenames = [os.path.join(os.getcwd(), 'data/18-83510-I94-Data-2016', fn) for fn in os.listdir('data/18-83510-I94-Data-2016')]\n",
    "sas_header_file = 'data/I94_SAS_Labels_Descriptions.SAS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/krchia/udacity-capstone/data/18-83510-I94-Data-2016/i94_nov16_sub.sas7bdat',\n",
       " '/Users/krchia/udacity-capstone/data/18-83510-I94-Data-2016/i94_dec16_sub.sas7bdat',\n",
       " '/Users/krchia/udacity-capstone/data/18-83510-I94-Data-2016/i94_sep16_sub.sas7bdat',\n",
       " '/Users/krchia/udacity-capstone/data/18-83510-I94-Data-2016/i94_aug16_sub.sas7bdat',\n",
       " '/Users/krchia/udacity-capstone/data/18-83510-I94-Data-2016/i94_may16_sub.sas7bdat',\n",
       " '/Users/krchia/udacity-capstone/data/18-83510-I94-Data-2016/i94_jun16_sub.sas7bdat',\n",
       " '/Users/krchia/udacity-capstone/data/18-83510-I94-Data-2016/i94_oct16_sub.sas7bdat',\n",
       " '/Users/krchia/udacity-capstone/data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat',\n",
       " '/Users/krchia/udacity-capstone/data/18-83510-I94-Data-2016/i94_jan16_sub.sas7bdat',\n",
       " '/Users/krchia/udacity-capstone/data/18-83510-I94-Data-2016/i94_jul16_sub.sas7bdat',\n",
       " '/Users/krchia/udacity-capstone/data/18-83510-I94-Data-2016/i94_mar16_sub.sas7bdat',\n",
       " '/Users/krchia/udacity-capstone/data/18-83510-I94-Data-2016/i94_feb16_sub.sas7bdat']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sas_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploratory_file = sas_filenames[0]\n",
    "df = pd.read_sas(exploratory_file, 'sas7bdat', encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I94YR : 4 digit year\n",
      "I94MON : Numeric month\n",
      "I94CIT & I94RES : This format shows all the valid and invalid codes for processing\n",
      "I94PORT : This format shows all the valid and invalid codes for processing\n",
      "I94MODE : There are missing values as well as not reported (9)\n",
      "I94BIR : Age of Respondent in Years\n",
      "COUNT : Used for summary statistics\n",
      "DTADFILE : Character Date Field - Date added to I-94 Files - CIC does not use\n",
      "VISAPOST : Department of State where where Visa was issued - CIC does not use\n",
      "OCCUP : Occupation that will be performed in U.S. - CIC does not use\n",
      "ENTDEPA : Arrival Flag - admitted or paroled into the U.S. - CIC does not use\n",
      "ENTDEPD : Departure Flag - Departed, lost I-94 or is deceased - CIC does not use\n",
      "ENTDEPU : Update Flag - Either apprehended, overstayed, adjusted to perm residence - CIC does not use\n",
      "MATFLAG : Match flag - Match of arrival and departure records\n",
      "BIRYEAR : 4 digit year of birth\n",
      "DTADDTO : Character Date Field - Date to which admitted to U.S. (allowed to stay until) - CIC does not use\n",
      "GENDER : Non-immigrant sex\n",
      "INSNUM : INS number\n",
      "AIRLINE : Airline used to arrive in U.S.\n",
      "ADMNUM : Admission Number\n",
      "FLTNO : Flight number of Airline used to arrive in U.S.\n",
      "VISATYPE : Class of admission legally admitting the non-immigrant to temporarily stay in U.S.\n"
     ]
    }
   ],
   "source": [
    "with open(sas_header_file) as f:\n",
    "    lines = f.readlines()    \n",
    "\n",
    "comments = [line for line in lines if '/*' in line and '*/\\n' in line]\n",
    "regexp = re.compile(r'^/\\*\\s+(?P<code>.+?)\\s+-\\s+(?P<description>.+)\\s+\\*/$')\n",
    "matches = [regexp.match(c) for c in comments]\n",
    "\n",
    "for m in matches:\n",
    "    print(m.group(\"code\"), \":\", m.group('description'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### i94_port_lines = (303, 962)\n",
    "re_obj = re.compile(r'\\'(.*)\\'.*\\'(.*)\\'')\n",
    "valid_ports = {}\n",
    "for line in lines[302:961]:\n",
    "    match = re_obj.search(line)\n",
    "    valid_ports[match.group(1)]=[match.group(2)]\n",
    "from pprint import pprint\n",
    "# pprint(valid_ports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2914926, 28)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>689.0</td>\n",
       "      <td>CHA</td>\n",
       "      <td>20759.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1954.0</td>\n",
       "      <td>02082017</td>\n",
       "      <td>F</td>\n",
       "      <td>2771</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.329581e+10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>NOG</td>\n",
       "      <td>20759.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>FL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1969.0</td>\n",
       "      <td>11222016</td>\n",
       "      <td>M</td>\n",
       "      <td>1163</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.361691e+10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>NOG</td>\n",
       "      <td>20759.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1982.0</td>\n",
       "      <td>03032017</td>\n",
       "      <td>M</td>\n",
       "      <td>1442</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.364268e+10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>65.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>NEW</td>\n",
       "      <td>20759.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UA</td>\n",
       "      <td>1.292481e+10</td>\n",
       "      <td>49</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>67.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>687.0</td>\n",
       "      <td>687.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>20759.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1993.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DL</td>\n",
       "      <td>1.292481e+10</td>\n",
       "      <td>110</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode i94addr  \\\n",
       "0    8.0  2016.0    11.0   126.0   689.0     CHA  20759.0      2.0     NaN   \n",
       "1   18.0  2016.0    11.0   582.0   582.0     NOG  20759.0      9.0      FL   \n",
       "2   19.0  2016.0    11.0   582.0   582.0     NOG  20759.0      9.0      CA   \n",
       "3   65.0  2016.0    11.0   213.0   213.0     NEW  20759.0      1.0      CA   \n",
       "4   67.0  2016.0    11.0   687.0   687.0     ATL  20759.0      1.0      AL   \n",
       "\n",
       "   depdate  ...  entdepu  matflag  biryear   dtaddto gender insnum airline  \\\n",
       "0      NaN  ...      NaN      NaN   1954.0  02082017      F   2771     NaN   \n",
       "1      NaN  ...      NaN        M   1969.0  11222016      M   1163     NaN   \n",
       "2      NaN  ...      NaN        M   1982.0  03032017      M   1442     NaN   \n",
       "3      NaN  ...      NaN      NaN   1990.0       D/S      M    NaN      UA   \n",
       "4      NaN  ...      NaN      NaN   1993.0       D/S      M    NaN      DL   \n",
       "\n",
       "         admnum fltno visatype  \n",
       "0  1.329581e+10   NaN       WT  \n",
       "1  1.361691e+10   NaN       CP  \n",
       "2  1.364268e+10   NaN       CP  \n",
       "3  1.292481e+10    49       F1  \n",
       "4  1.292481e+10   110       F1  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Comments below. We will ultimately use Spark for processing in the data pipeline, but will use Pandas to explore the data first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['cicid', 'i94yr', 'i94mon', 'i94cit', 'i94res', 'i94port', 'arrdate',\n",
       "       'i94mode', 'i94addr', 'depdate', 'i94bir', 'i94visa', 'count',\n",
       "       'dtadfile', 'visapost', 'occup', 'entdepa', 'entdepd', 'entdepu',\n",
       "       'matflag', 'biryear', 'dtaddto', 'gender', 'insnum', 'airline',\n",
       "       'admnum', 'fltno', 'visatype'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1954., 1969., 1982., 1990., 1993., 1978., 1967., 2000., 1985.,\n",
       "       1981., 1984., 1989., 1999., 1975., 1986., 1992., 1943., 1980.,\n",
       "       1948., 1965., 1997., 1950., 1949., 1996., 1991., 1964., 1988.,\n",
       "       1962., 1955., 1939., 1972., 1970., 1977., 1974., 1963., 1956.,\n",
       "       1995., 1994., 1951., 1966., 1961., 1979., 1958., 1987., 1959.,\n",
       "       1983., 1971., 1936., 1968., 1946., 1960., 1947., 1976., 1944.,\n",
       "       1957., 2001., 1998., 2011., 1952., 2005., 2002., 1953., 1973.,\n",
       "       1929., 1941., 2008., 1945., 2010., 2014., 2015., 2012., 1940.,\n",
       "       2007., 1937., 2006., 2009., 2004., 1942., 2016., 1938., 2003.,\n",
       "       2013.,   nan, 1932., 1933., 1928., 1934., 1919., 1935., 1931.,\n",
       "       1925., 1930., 1927., 1924., 1920., 1926., 1921., 1918., 1922.,\n",
       "       1923., 1917., 1904., 1912., 1901., 2017., 1916., 1911., 1906.,\n",
       "       1905.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.biryear.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 62.,  47.,  34.,  26.,  23.,  38.,  49.,  16.,  31.,  35.,  32.,\n",
       "        27.,  17.,  41.,  30.,  24.,  73.,  36.,  68.,  51.,  19.,  66.,\n",
       "        67.,  20.,  25.,  52.,  28.,  54.,  61.,  77.,  44.,  46.,  39.,\n",
       "        42.,  53.,  60.,  21.,  22.,  65.,  50.,  55.,  37.,  58.,  29.,\n",
       "        57.,  33.,  45.,  80.,  48.,  70.,  56.,  69.,  40.,  72.,  59.,\n",
       "        15.,  18.,   5.,  64.,  11.,  14.,  63.,  43.,  87.,  75.,   8.,\n",
       "        71.,   6.,   2.,   1.,   4.,  76.,   9.,  79.,  10.,   7.,  12.,\n",
       "        74.,   0.,  78.,  13.,   3.,  nan,  84.,  83.,  88.,  82.,  97.,\n",
       "        81.,  85.,  91.,  86.,  89.,  92.,  96.,  90.,  95.,  98.,  94.,\n",
       "        93.,  99., 112., 104., 115.,  -1., 100., 105., 110., 111.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.i94bir.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CHA', 'NOG', 'NEW', 'ATL', 'BOS', 'MIA', 'FTL', 'NYC', 'DAL',\n",
       "       'HOU', 'ORL', 'LOS', 'SFR', 'PHI', 'CHI', 'SAJ', 'WAS', 'DET',\n",
       "       'SFB', 'SPM', 'AGA', 'SAI', 'HHW', 'DEN', 'LVG', 'NOL', 'SEA',\n",
       "       'TAM', 'RDU', 'SDP', 'SAC', 'PHO', 'BAL', 'ELP', 'ONT', 'AUS',\n",
       "       'SLC', 'SNJ', 'NCA', 'XXX', 'SNA', 'STT', 'FMY', 'PVD', 'TST',\n",
       "       'CLT', 'OAK', 'SHA', 'MAA', 'CHM', 'PEV', 'NYL', 'LAR', 'OTT',\n",
       "       'BRO', 'SYS', 'FPT', 'BUF', 'SAV', 'TUC', 'CLE', 'NSV', 'OGG',\n",
       "       'VCV', 'X96', 'WPB', 'HAR', 'CLM', 'TOR', 'POO', 'CIN', 'W55',\n",
       "       'PIT', 'STL', 'MDT', 'PSP', 'OTM', 'TEC', 'PEM', 'BLA', 'LNB',\n",
       "       'DOU', 'GAL', 'ABQ', 'MCA', 'OPF', '5T6', 'ANC', 'KAN', 'MMU',\n",
       "       'RFD', 'FPR', 'CLS', 'INT', 'RNO', 'CHL', 'SYR', 'ROC', 'HPN',\n",
       "       'BED', 'INP', 'MIL', 'HIG', 'LAU', 'SGR', 'BGM', 'GRB', 'PTK',\n",
       "       'SRQ', 'CRQ', 'DAB', 'JAC', 'SUM', 'YGF', 'DUB', 'OKC', 'SAA',\n",
       "       'KEY', 'ALB', 'NOR', 'AXB', 'CRP', 'ORO', 'SWE', 'MEM', 'FAJ',\n",
       "       'MON', 'CHR', 'FAR', 'KOA', 'DER', 'FOK', 'NAS', 'VNY', 'ADS',\n",
       "       'DLR', 'CAL', 'STR', 'HID', 'PHU', 'FRB', 'CHS', 'CLG', 'MTH',\n",
       "       'JMZ', 'LYN', 'LEX', 'HAL', 'DOV', 'X44', 'FRE', 'MLB', 'MHT',\n",
       "       'SWF', 'BQN', 'OMA', 'BRG', 'GRP', 'YIP', 'HTM', 'MOB', 'CXO',\n",
       "       'VIC', 'PON', 'CRA', 'SPO', 'WIN', 'SNN', 'SPE', 'AGU', 'PSM',\n",
       "       'GRF', 'LIH', 'WIL', 'APF', 'RST', 'NIA', 'OGD', 'EDA', 'ICT',\n",
       "       'BHX', 'PNG', 'HLG', 'GSP', 'ADW', 'ATW', 'FBA', 'PRO', 'ROS',\n",
       "       'JKM', 'DSM', 'PCW', 'PHF', 'MAF', 'ABS', 'PIE', 'HAM', 'MOR',\n",
       "       'RYY', 'SDY', 'HEL', 'PIR', 'CAK', 'JFA', 'RCM', 'DPA', 'REN',\n",
       "       'BTN', 'LAN', 'ERI', 'LON', 'SUS', 'EPI', 'LEW', 'PBB', 'THO',\n",
       "       'ANZ', 'BOA', 'EGP', 'FTC', 'GPM', 'LLB', 'MAS', 'MOO', 'NRG',\n",
       "       'PDN', 'PGR', 'PHR', 'POR', 'PRE', 'ROO', 'SKA', 'TRO', 'WBE',\n",
       "       'YSL', 'AGN', 'AND', 'CNA', 'COB', 'DAC', 'FWA', 'MET', 'RIF',\n",
       "       'ROM', 'SSM', 'ADT', 'LCB', 'LUK', 'BWA', 'LOI', 'MRC', 'NAC',\n",
       "       'COL', 'FAL', 'FER', 'ABG', 'ALC', 'BEB', 'BEE', 'CHT', 'DNA',\n",
       "       'FRT', 'FTK', 'MAD', 'NIG', 'NRN', 'NRT', 'PNH', 'PTL', 'RAY',\n",
       "       'ROU', 'VIB', 'VNB', 'BWM', 'DLB', 'DNS', 'DVL', 'HNS', 'MAI',\n",
       "       'NEC', 'NOO', 'SJO', 'SLU', 'VCB', 'WAL', 'WAR', 'CRY', 'SHR',\n",
       "       'TUR', 'WHO', 'FAB', 'SAR', 'LWT', 'SAS', 'FTF', 'BAU', 'ANT',\n",
       "       'HVR', 'LUB', 'RIO'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.i94port.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the I94 immigration data, we can clean the following columns: \n",
    "\n",
    "Clean `biryear` by ensuring data values not existing between 1900 and 2016 are turned to NULL. \n",
    "\n",
    "Similarly for `i194bir` with values outside 0 to 116. For `i94port`, drop rows with 'XXX' (unknown as specified in header file)\n",
    "\n",
    "Also, not all columns are relevant. Keep only those that will be used in the dimension table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "def file_to_spark_df(file):\n",
    "    return spark.read.format('com.github.saurfang.sas.spark').load(file)\n",
    "\n",
    "def clean_i94_data(df):\n",
    "    columns_to_keep = {'i94yr', 'i94mon', 'i94cit', 'i94port', 'i94mode', 'i94bir', 'arrdate', 'depdate', 'i94visa'}\n",
    "    temp = df.filter(df.i94port.isin(list(valid_ports.keys())))\n",
    "    return temp.select([c for c in temp.columns if c in columns_to_keep])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+-------+-------+-------+-------+------+-------+\n",
      "| i94yr|i94mon|i94cit|i94port|arrdate|i94mode|depdate|i94bir|i94visa|\n",
      "+------+------+------+-------+-------+-------+-------+------+-------+\n",
      "|2016.0|  11.0| 126.0|    CHA|20759.0|    2.0|   null|  62.0|    2.0|\n",
      "|2016.0|  11.0| 582.0|    NOG|20759.0|    9.0|   null|  47.0|    2.0|\n",
      "|2016.0|  11.0| 582.0|    NOG|20759.0|    9.0|   null|  34.0|    2.0|\n",
      "|2016.0|  11.0| 213.0|    NEW|20759.0|    1.0|   null|  26.0|    3.0|\n",
      "|2016.0|  11.0| 687.0|    ATL|20759.0|    1.0|   null|  23.0|    3.0|\n",
      "|2016.0|  11.0| 254.0|    BOS|20759.0|    1.0|20828.0|  38.0|    3.0|\n",
      "|2016.0|  11.0| 692.0|    MIA|20759.0|    1.0|20768.0|  49.0|    1.0|\n",
      "|2016.0|  11.0| 690.0|    ATL|20759.0|    1.0|20848.0|  16.0|    2.0|\n",
      "|2016.0|  11.0| 696.0|    ATL|20759.0|    1.0|20937.0|  31.0|    2.0|\n",
      "|2016.0|  11.0| 692.0|    FTL|20759.0|    1.0|   null|  35.0|    2.0|\n",
      "|2016.0|  11.0| 687.0|    MIA|20759.0|    1.0|20764.0|  38.0|    2.0|\n",
      "|2016.0|  11.0| 689.0|    MIA|20759.0|    1.0|   null|  32.0|    3.0|\n",
      "|2016.0|  11.0| 254.0|    NYC|20759.0|    1.0|20763.0|  26.0|    2.0|\n",
      "|2016.0|  11.0| 245.0|    NYC|20759.0|    1.0|20772.0|  27.0|    2.0|\n",
      "|2016.0|  11.0| 691.0|    MIA|20759.0|    1.0|20768.0|  17.0|    2.0|\n",
      "|2016.0|  11.0| 504.0|    FTL|20759.0|    1.0|20763.0|  31.0|    2.0|\n",
      "|2016.0|  11.0| 689.0|    MIA|20759.0|    1.0|20763.0|  41.0|    2.0|\n",
      "|2016.0|  11.0| 687.0|    DAL|20759.0|    1.0|20764.0|  34.0|    2.0|\n",
      "|2016.0|  11.0| 692.0|    DAL|20759.0|    1.0|20762.0|  30.0|    2.0|\n",
      "|2016.0|  11.0| 582.0|    HOU|20759.0|    1.0|20809.0|  24.0|    2.0|\n",
      "+------+------+------+-------+-------+-------+-------+------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration_df = clean_i94_data(file_to_spark_df(sas_filenames[0]))\n",
    "immigration_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.read_csv('data/GlobalLandTemperaturesByCity.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1743-11-01</td>\n",
       "      <td>6.068</td>\n",
       "      <td>1.737</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1743-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1744-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1744-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1744-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  AverageTemperature  AverageTemperatureUncertainty   City  \\\n",
       "0  1743-11-01               6.068                          1.737  Århus   \n",
       "1  1743-12-01                 NaN                            NaN  Århus   \n",
       "2  1744-01-01                 NaN                            NaN  Århus   \n",
       "3  1744-02-01                 NaN                            NaN  Århus   \n",
       "4  1744-03-01                 NaN                            NaN  Århus   \n",
       "\n",
       "   Country Latitude Longitude  \n",
       "0  Denmark   57.05N    10.33E  \n",
       "1  Denmark   57.05N    10.33E  \n",
       "2  Denmark   57.05N    10.33E  \n",
       "3  Denmark   57.05N    10.33E  \n",
       "4  Denmark   57.05N    10.33E  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39.650999999999996\n",
      "-42.70399999999999\n"
     ]
    }
   ],
   "source": [
    "unique_temps = temp_df.AverageTemperature.unique()\n",
    "print(max(unique_temps))\n",
    "print(min(unique_temps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the temperature data, we can clean the following columns:\n",
    "\n",
    "Values for `AverageTemperatures` seem to be reasonable so no need to remove outliers\n",
    "\n",
    "Drop rows with `AverageTemperature == NaN`\n",
    "\n",
    "Drop rows with duplicate locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf()\n",
    "def city_to_port(city):\n",
    "    for key in valid_port:\n",
    "        if city.lower() in valid_port[key][0].lower():\n",
    "            return key\n",
    "\n",
    "def csv_to_spark_df(file):\n",
    "    return spark.read.format(\"csv\").option(\"header\", \"true\").load(file)\n",
    "\n",
    "def clean_temp_data(temp_df):\n",
    "    temp_df = temp_df.filter(temp_df.AverageTemperature != 'NaN')\n",
    "    temp_df = temp_df.dropDuplicates(['City', 'Country'])\n",
    "    temp_df = temp_df.withColumn(\"i94port\", city_to_port(temp_df.City))\n",
    "    return temp_df.filter(temp_df.i94port != 'null')\n",
    "\n",
    "temp_df = clean_temp_data(csv_to_spark_df(\"data/GlobalLandTemperaturesByCity.csv\"))\n",
    "# temp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env\n",
    "%env IPYTHON=1\n",
    "%env PYSPARK_PYTHON=/usr/local/bin/python3\n",
    "%env PYSPARK_DRIVER_PYTHON=/usr/local/bin/python3\n",
    "%env PYSPARK_DRIVER_PYTHON_OPTS=\"notebook\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "\n",
    "#### 3.1 Conceptual Data Model\n",
    "\n",
    "Dimension Tables\n",
    "\n",
    "`dim_demographics` will contain the following columns from the I94 data. \n",
    "\n",
    "- `I94YR` : 4 digit year\n",
    "- `I94MON` : Numeric month\n",
    "- `I94CIT` : This format shows all the valid and invalid codes for processing\n",
    "- `I94PORT` : This format shows all the valid and invalid codes for processing\n",
    "- `I94MODE` : There are missing values as well as not reported (9)\n",
    "- `I94BIR` : Age of Respondent in Years\n",
    "- `ARRDATE` : Arrival date\n",
    "- `DEPDATE` : Departure date\n",
    "- `I94VISA` : Visa code (Business/Pleasure/Student)\n",
    "\n",
    "`dim_temperature` will contain the following columns from the temperature dataset. Most are self explanatory names.\n",
    "\n",
    "- `I94PORT` : map the city/country/location to the corresponding I94 port code.\n",
    "- `AverageTemperature`\n",
    "- `City`\n",
    "- `Country`\n",
    "- `Latitude`\n",
    "- `Longitude`\n",
    "\n",
    "Fact Tables\n",
    "\n",
    "`fact_immigration` allows queries in line with the intended purpose of the project:\n",
    "\n",
    "- `I94YR` : 4 digit year\n",
    "- `I94MON` : Numeric month\n",
    "- `I94CIT` : This format shows all the valid and invalid codes for processing\n",
    "- `I94PORT` : This format shows all the valid and invalid codes for processing\n",
    "- `I94MODE` : There are missing values as well as not reported (9)\n",
    "- `I94BIR` : Age of Respondent in Years\n",
    "- `ARRDATE` : Arrival date\n",
    "- `DEPDATE` : Departure date\n",
    "- `I94VISA` : Visa code (Business/Pleasure/Student)\n",
    "- `AverageTemperature`\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "\n",
    "1. clean the data and create the dimension table for both i94 and temperature datasets\n",
    "2. create fact table by joining both dimension tables on i94port and write to parquet file partitioned by i94port"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Step 4: Run Pipelines to Model the Data\n",
    "#### 4.1 Create the data model\n",
    "\n",
    "Build the data pipelines to create the data model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "immigration_df = clean_i94_data(file_to_spark_df(sas_filenames[0]))\n",
    "immigration_df.write \\\n",
    "    .mode(\"append\") \\\n",
    "    .partitionBy(\"i94port\") \\\n",
    "    .parquet(\"/tables/immigration.parquet\")\n",
    "\n",
    "temp_df = clean_temp_data(csv_to_spark_df(\"data/GlobalLandTemperaturesByCity.csv\"))\n",
    "temp_df.write \\\n",
    "    .mode(\"append\") \\\n",
    "    .partitionBy(\"i94port\") \\\n",
    "    .parquet(\"/tables/temperature.parquet\")\n",
    "\n",
    "# Create temporary views of the immigration and temperature data\n",
    "immigration_df.createOrReplaceTempView(\"immigration_view\")\n",
    "temp_df.createOrReplaceTempView(\"temp_view\")\n",
    "\n",
    "# Create the fact table by joining the immigration and temperature views\n",
    "fact_table = spark.sql(\"\"\"\n",
    "SELECT immigration_view.i94yr as year,\n",
    "       immigration_view.i94mon as month,\n",
    "       immigration_view.i94cit as city,\n",
    "       immigration_view.i94port as i94port,\n",
    "       immigration_view.i94mode as i94mode,\n",
    "       immigration_view.i94bir as i94bir,\n",
    "       immigration_view.arrdate as arrival_date,\n",
    "       immigration_view.depdate as departure_date,\n",
    "       immigration_view.i94visa as reason,\n",
    "       temp_view.AverageTemperature as temperature,\n",
    "FROM immigration_view\n",
    "JOIN temp_view ON (immigration_view.i94port = temp_view.i94port)\n",
    "\"\"\")\n",
    "\n",
    "# Write fact table to parquet files partitioned by i94port\n",
    "fact_table.write \\\n",
    "    .mode(\"append\") \\\n",
    "    .partitionBy(\"i94port\") \\\n",
    "    .parquet(\"/results/fact.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_check(df):\n",
    "    return df.count() == 0 \n",
    "\n",
    "def integrity_check(df_immigration, df_temp):\n",
    "    return df_immigration.select(col(\"i94port\")).distinct() \\\n",
    "         .join(df_temp, df_immigration[\"i94port\"] == df_temp[\"i94port\"], \"left_anti\") \\\n",
    "         .count() == 0\n",
    "\n",
    "def quality_check(df_immigration, df_temp):\n",
    "    return count_check(df_immigration) and count_check(df_temp) \\\n",
    "        and integrity_check(df_immigration, df_temp)\n",
    "\n",
    "# Perform data quality check\n",
    "quality_check(df_immigration, df_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary\n",
    "\n",
    "Dimension Tables\n",
    "\n",
    "`dim_demographics` will contain the following columns from the I94 data. \n",
    "\n",
    "- `I94YR` : 4 digit year\n",
    "- `I94MON` : Numeric month\n",
    "- `I94CIT` : This format shows all the valid and invalid codes for processing\n",
    "- `I94PORT` : This format shows all the valid and invalid codes for processing\n",
    "- `I94MODE` : There are missing values as well as not reported (9)\n",
    "- `I94BIR` : Age of Respondent in Years\n",
    "- `ARRDATE` : Arrival date\n",
    "- `DEPDATE` : Departure date\n",
    "- `I94VISA` : Visa code (Business/Pleasure/Student)\n",
    "\n",
    "`dim_temperature` will contain the following columns from the temperature dataset. Most are self explanatory names.\n",
    "\n",
    "- `I94PORT` : map the city/country/location to the corresponding I94 port code.\n",
    "- `AverageTemperature`\n",
    "- `City`\n",
    "- `Country`\n",
    "- `Latitude`\n",
    "- `Longitude`\n",
    "\n",
    "Fact Tables\n",
    "\n",
    "`fact_immigration` allows queries in line with the intended purpose of the project:\n",
    "\n",
    "- `I94YR` : 4 digit year\n",
    "- `I94MON` : Numeric month\n",
    "- `I94CIT` : This format shows all the valid and invalid codes for processing\n",
    "- `I94PORT` : This format shows all the valid and invalid codes for processing\n",
    "- `I94MODE` : There are missing values as well as not reported (9)\n",
    "- `I94BIR` : Age of Respondent in Years\n",
    "- `ARRDATE` : Arrival date\n",
    "- `DEPDATE` : Departure date\n",
    "- `I94VISA` : Visa code (Business/Pleasure/Student)\n",
    "- `AverageTemperature`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "\n",
    "Pandas was used to explore the data since it has an intuitive and easy to use API. For the production side of things (building the data pipeline), Spark was chosen since it can handle large amounts of data simply by scaling up the hardware. Spark can be unit tested locally with Hive support and in production to save to S3.\n",
    "\n",
    "* Propose how often the data should be updated and why.\n",
    "\n",
    "It depends on the business requirements and use case. If there is no hard requirements, then monthly would be best in conjunction with the file format.\n",
    "\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " \n",
    " Scale up the number of spark workers processing the data. Since the data is processed as a batch job every month, the spark workers can be provisioned only as necessary with an autoscaling service.\n",
    " \n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " \n",
    " If the data is updated with new data every day, then the batch job can be scheduled with a cron job. If other data operations are involved, a more configurable tool like Airflow can help to manage the data pipeline.\n",
    " \n",
    " * The database needed to be accessed by 100+ people.\n",
    " \n",
    " This would depend on the Spark settings. For example, the default Spark driver used for JDBC connections is Derby, which does not support concurrent writes. Hence one would have to tweak the Spark settings to support concurrent reads.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
